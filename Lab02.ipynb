{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据预处理和划分\n",
    "1. 首先，确保有一个中文分词标注数据集。\n",
    "2. 这个数据集应该包括已经分好词的句子，每个词用空格隔开。例如：\n",
    "3. 去除标点符号。\n",
    "4. 训练集/测试集比率为7:3\n",
    "```bash\n",
    "我 爱 北京 天安门\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24019\n",
      "[('的', 99718), ('了', 22500), ('在', 22415), ('和', 20074), ('是', 17908)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "import random\n",
    "\n",
    "\n",
    "# 加载数据集并去除标点符号，确保每个句子至少有一个字\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    cleaned_lines = [re.sub(r'[^\\w\\s]', '', line).strip() for line in lines]\n",
    "    # 过滤掉空行\n",
    "    cleaned_lines = [line for line in cleaned_lines if len(line) > 0]\n",
    "    return cleaned_lines\n",
    "\n",
    "# 将数据集划分为训练集和测试集（70% : 30%）\n",
    "def split_data(lines, train_ratio=0.7):\n",
    "    random.shuffle(lines)\n",
    "    split_point = int(len(lines) * train_ratio)\n",
    "    return lines[:split_point], lines[split_point:]\n",
    "\n",
    "# 数据集文件路径\n",
    "file_path = 'RenMinData.txt'\n",
    "lines = load_data(file_path)\n",
    "train_lines, test_lines = split_data(lines)\n",
    "\n",
    "# 构建词频词典\n",
    "def build_vocab(lines):\n",
    "    word_freq = collections.Counter()\n",
    "    for line in lines:\n",
    "        words = line.strip().split()\n",
    "        word_freq.update(words)\n",
    "    return word_freq\n",
    "\n",
    "vocab = build_vocab(train_lines)\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print(len(sorted_vocab))\n",
    "print(sorted_vocab[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建HMM模型\n",
    "1. 定义状态和观测值\n",
    "2. 计算初始概率、转移概率和发射概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义状态\n",
    "states = ['B', 'M', 'E', 'S']\n",
    "\n",
    "# 初始化概率矩阵\n",
    "initial_prob = {state: 0 for state in states}\n",
    "transition_prob = {state: {state_: 0 for state_ in states} for state in states}\n",
    "emission_prob = {state: collections.Counter() for state in states}\n",
    "\n",
    "# 统计频率并训练 HMM 模型\n",
    "def train_hmm(lines):\n",
    "    for line in lines:\n",
    "        words = line.strip().split()\n",
    "        if not words:\n",
    "            continue\n",
    "        line_states = []\n",
    "        for word in words:\n",
    "            if len(word) == 1:\n",
    "                line_states.append('S')\n",
    "            else:\n",
    "                line_states.extend(['B'] + ['M'] * (len(word) - 2) + ['E'])\n",
    "        \n",
    "        initial_prob[line_states[0]] += 1\n",
    "        \n",
    "        for i in range(len(line_states) - 1):\n",
    "            transition_prob[line_states[i]][line_states[i + 1]] += 1\n",
    "        \n",
    "        for word, state_seq in zip(words, line_states):\n",
    "            for char, state in zip(word, state_seq):\n",
    "                emission_prob[state][char] += 1\n",
    "\n",
    "train_hmm(train_lines)\n",
    "\n",
    "# 转换为概率\n",
    "initial_prob = {state: count / sum(initial_prob.values()) for state, count in initial_prob.items()}\n",
    "transition_prob = {state: {state_: count / sum(transition_prob[state].values()) for state_, count in state_dict.items()} for state, state_dict in transition_prob.items()}\n",
    "emission_prob = {state: {char: count / sum(emission_prob[state].values()) for char, count in char_dict.items()} for state, char_dict in emission_prob.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_prob: \n",
      "{\n",
      "  \"B\": 0.5954871352180898,\n",
      "  \"M\": 0.0,\n",
      "  \"E\": 0.0,\n",
      "  \"S\": 0.4045128647819102\n",
      "}\n",
      "transition_prob: \n",
      "{\n",
      "  \"B\": {\n",
      "    \"B\": 0.0,\n",
      "    \"M\": 0.11658781137718696,\n",
      "    \"E\": 0.883412188622813,\n",
      "    \"S\": 0.0\n",
      "  },\n",
      "  \"M\": {\n",
      "    \"B\": 0.0,\n",
      "    \"M\": 0.2779321774604793,\n",
      "    \"E\": 0.7220678225395206,\n",
      "    \"S\": 0.0\n",
      "  },\n",
      "  \"E\": {\n",
      "    \"B\": 0.5837256220578345,\n",
      "    \"M\": 0.0,\n",
      "    \"E\": 0.0,\n",
      "    \"S\": 0.41627437794216543\n",
      "  },\n",
      "  \"S\": {\n",
      "    \"B\": 0.47243960597041273,\n",
      "    \"M\": 0.0,\n",
      "    \"E\": 0.0,\n",
      "    \"S\": 0.5275603940295873\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(\"initial_prob: \")\n",
    "print(json.dumps(initial_prob, indent=2))\n",
    "\n",
    "print(\"transition_prob: \")\n",
    "print(json.dumps(transition_prob, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 实现基线分词方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正向最大匹配法（FMM）\n",
    "def fmm_segment(sentence, vocab):\n",
    "    max_len = max(len(word) for word in vocab)\n",
    "    words = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        for j in range(min(max_len, len(sentence) - i), 0, -1):\n",
    "            word = sentence[i:i + j]\n",
    "            if word in vocab:\n",
    "                words.append(word)\n",
    "                i += j\n",
    "                break\n",
    "        else:\n",
    "            words.append(sentence[i])\n",
    "            i += 1\n",
    "    return words\n",
    "\n",
    "# 逆向最大匹配法（BMM）\n",
    "def bmm_segment(sentence, vocab):\n",
    "    max_len = max(len(word) for word in vocab)\n",
    "    words = []\n",
    "    i = len(sentence)\n",
    "    while i > 0:\n",
    "        for j in range(min(max_len, i), 0, -1):\n",
    "            word = sentence[i - j:i]\n",
    "            if word in vocab:\n",
    "                words.insert(0, word)\n",
    "                i -= j\n",
    "                break\n",
    "        else:\n",
    "            words.insert(0, sentence[i - 1])\n",
    "            i -= 1\n",
    "    return words\n",
    "\n",
    "# 双向最大匹配法（BiMM）\n",
    "def bimm_segment(sentence, vocab):\n",
    "    fmm_result = fmm_segment(sentence, vocab)\n",
    "    bmm_result = bmm_segment(sentence, vocab)\n",
    "    if len(fmm_result) < len(bmm_result):\n",
    "        return fmm_result\n",
    "    elif len(fmm_result) > len(bmm_result):\n",
    "        return bmm_result\n",
    "    else:\n",
    "        if sum(len(word) for word in fmm_result) > sum(len(word) for word in bmm_result):\n",
    "            return fmm_result\n",
    "        else:\n",
    "            return bmm_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 实现 HMM 分词方法（使用维特比算法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    for y in states:\n",
    "        V[0][y] = start_p[y] * emit_p[y].get(sentence[0], 1e-8)\n",
    "        path[y] = [y]\n",
    "\n",
    "    for t in range(1, len(sentence)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "\n",
    "        for y in states:\n",
    "            (prob, state) = max((V[t-1][y0] * trans_p[y0].get(y, 1e-8) * emit_p[y].get(sentence[t], 1e-8), y0) for y0 in states)\n",
    "            V[t][y] = prob\n",
    "            newpath[y] = path[state] + [y]\n",
    "\n",
    "        path = newpath\n",
    "\n",
    "    (prob, state) = max((V[-1][y], y) for y in states)\n",
    "    return prob, path[state]\n",
    "\n",
    "def segment(sentence):\n",
    "    prob, pos_list = viterbi(sentence, states, initial_prob, transition_prob, emission_prob)\n",
    "    result = []\n",
    "    begin, next = 0, 0\n",
    "    for i, char in enumerate(sentence):\n",
    "        pos = pos_list[i]\n",
    "        if pos == 'B':\n",
    "            begin = i\n",
    "        elif pos == 'E':\n",
    "            result.append(sentence[begin:i+1])\n",
    "            next = i + 1\n",
    "        elif pos == 'S':\n",
    "            result.append(char)\n",
    "            next = i + 1\n",
    "    if next < len(sentence):\n",
    "        result.append(sentence[next:])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 实现评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data, segment_func):\n",
    "    TP, FP, FN = 0, 0, 0\n",
    "\n",
    "    for line in test_data:\n",
    "        line = re.sub(r'[^\\w\\s]', '', line).strip()\n",
    "        words = line.strip().split()\n",
    "        sentence = ''.join(words)\n",
    "        segmented = segment_func(sentence)\n",
    "        \n",
    "        gold = set(words)\n",
    "        pred = set(segmented)\n",
    "        \n",
    "        TP += len(gold & pred)\n",
    "        FP += len(pred - gold)\n",
    "        FN += len(gold - pred)\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 对比结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM Model - Precision: 0.4491, Recall: 0.4390, F1-Score: 0.4440\n",
      "FMM - Precision: 0.9805, Recall: 0.9806, F1-Score: 0.9806\n",
      "BMM - Precision: 0.9793, Recall: 0.9797, F1-Score: 0.9795\n",
      "BiMM - Precision: 0.9807, Recall: 0.9805, F1-Score: 0.9806\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 评估 HMM 分词方法\n",
    "precision_hmm, recall_hmm, f1_hmm = evaluate(test_lines, segment)\n",
    "print(f'HMM Model - Precision: {precision_hmm:.4f}, Recall: {recall_hmm:.4f}, F1-Score: {f1_hmm:.4f}')\n",
    "\n",
    "# 评估正向最大匹配法（FMM）\n",
    "precision_fmm, recall_fmm, f1_fmm = evaluate(test_lines, lambda x: fmm_segment(x, vocab))\n",
    "print(f'FMM - Precision: {precision_fmm:.4f}, Recall: {recall_fmm:.4f}, F1-Score: {f1_fmm:.4f}')\n",
    "\n",
    "# 评估逆向最大匹配法（BMM）\n",
    "precision_bmm, recall_bmm, f1_bmm = evaluate(test_lines, lambda x: bmm_segment(x, vocab))\n",
    "print(f'BMM - Precision: {precision_bmm:.4f}, Recall: {recall_bmm:.4f}, F1-Score: {f1_bmm:.4f}')\n",
    "\n",
    "# 评估双向最大匹配法（BiMM）\n",
    "precision_bimm, recall_bimm, f1_bimm = evaluate(test_lines, lambda x: bimm_segment(x, vocab))\n",
    "print(f'BiMM - Precision: {precision_bimm:.4f}, Recall: {recall_bimm:.4f}, F1-Score: {f1_bimm:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
